{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ed3e340-17fd-4b71-a98e-c776aa45d053",
   "metadata": {},
   "source": [
    "<!-- DATA PROVIDER INSTRUCTIONS\n",
    "\n",
    "1. Provide the name of your dataset, replacing the bracketed placeholder text.\n",
    "2. Update the Registry of Open Data landing page URL, by replacing the bracketed placeholder text. The [REGISTRY_YAML_NAME] will correspond to the name of the YAML document in your pull request to the Registry of Open Data on Github, minus the .yaml file extension.\n",
    "3. Remove these comment blocks when you have completed each section.\n",
    "\n",
    "DATA PROVIDER INSTRUCTIONS -->\n",
    "\n",
    "# Get to Know a Dataset: CCRS MODIS Albedo at 250-m resolution and 10-day intervals\n",
    "\n",
    "This notebook serves as a guided tour of the [CCRS MODIS Albedo at 250-m resolution and 10-day intervals ](https://registry.opendata.aws/[REGISTRY_YAML_NAME]) dataset. More usage examples, tutorials, and documentation for this dataset and others can be found at the [Registry of Open Data on AWS](https://registry.opendata.aws/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3779654-eeee-4708-83cf-245e03303475",
   "metadata": {},
   "source": [
    "<!-- DATA PROVIDER INSTRUCTIONS\n",
    "\n",
    "The goal of this section is to orient users to the structure of your dataset. \n",
    "\n",
    "1. How are key prefixes and objects organized in your S3 bucket?\n",
    "2. What kinds of filetypes are represented in your dataset?\n",
    "3. Explain with text what users are expected to encounter, and then demonstrate with code the organizational framework you applied when creating your dataset.\n",
    "4. The responses to each question section are meant to be expanded or replaced as dictated by your dataset\n",
    "\n",
    "DATA PROVIDER INSTRUCTIONS -->\n",
    "\n",
    "### Q: How have you organized your dataset? Help us understand the key prefix structure of your S3 bucket.\n",
    "\n",
    "\n",
    "A descriptive documentation describes how the data is organized, how users can find the data they need, and how users can use the data.\n",
    "\n",
    "Full documentation for this dataset can be found at: https://data.eodms-sgdot.nrcan-rncan.gc.ca/public/CCRS/Trishchenko_MODIS_Albedo/\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49cd7832-0c47-4890-a597-de29ca73d907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODING GUIDELINES FOR DATA PROVIDER\n",
    "#\n",
    "# General notebook coding guidelines:\n",
    "# 1. Assume that your reader understands the basics of Jupyter Notebooks, Python, and their Python environment.\n",
    "#    The focus of this tutorial is on your dataset.\n",
    "# 2. For library requirements, list the required libraries in a comment block in \"requirements.txt\" format\n",
    "#    (https://pip.pypa.io/en/stable/reference/requirements-file-format/)\n",
    "# 3. Demonstrate importing libraries with the assumption that the user has correctly installed the required\n",
    "#    libraries.\n",
    "# 4. List and load all library dependencies once, at this point of the notebook, unless a complicated dependency\n",
    "#    set makes it unweildy.\n",
    "# 5. Remember, the goal of this tutorial is a 101-level introduction to your dataset using common tools and libraries.\n",
    "#    Examples using specialized environments and deep-diving methods are better suited to follow-up tutorials.\n",
    "#\n",
    "# CODING GUIDELINES FOR DATA PROVIDER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b47b69",
   "metadata": {},
   "source": [
    "EXAMPLE - REPLACE\n",
    "\n",
    "First we will import the Python libraries required throughout this notebook.\n",
    "\n",
    "EXAMPLE - REPLACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65803f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### EXAMPLE - REPLACE\n",
    "\n",
    "\n",
    "# This notebook requires the following additional libraries\n",
    "# (please install using the preferred method for your environment, e.g. pip, conda):\n",
    "#\n",
    "# boto3 >= 1.38.23\n",
    "# polars >= 1.30.0\n",
    "# matplotlib >= 3.10.3 \n",
    "\n",
    "# Import the libraries required for this notebook\n",
    "# Built-ins\n",
    "import json\n",
    "from pprint import pprint\n",
    "# Installed libraries\n",
    "import boto3, polars, matplotlib.pyplot as plt\n",
    "from botocore import UNSIGNED\n",
    "from botocore.config import Config\n",
    "\n",
    "### EXAMPLE - REPLACE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b14ae10",
   "metadata": {},
   "source": [
    "EXAMPLE - REPLACE\n",
    "\n",
    "Next, we will define the location of our dataset, create our boto3 S3 client, and list the top level prefixes in our S3 bucket. Here we see there is only one top-level prefix in our bucket.\n",
    "\n",
    "EXAMPLE - REPLACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be33d211",
   "metadata": {},
   "outputs": [],
   "source": [
    "### EXAMPLE - REPLACE\n",
    "\n",
    "# Location of the S3 bucket for this dataset\n",
    "bucket = \"amazon-last-mile-challenges\"\n",
    "\n",
    "# List the top level of the bucket using boto3. Because this is a public bucket, we don't need to sign requests.\n",
    "# Here we set the signature version to unsigned, which is required for public buckets.\n",
    "s3 = boto3.client('s3', config=Config(signature_version=UNSIGNED))\n",
    "\n",
    "# Print the items in the top-level prefixes\n",
    "for item in s3.list_objects_v2(Bucket=bucket, Delimiter='/')['CommonPrefixes']:\n",
    "    print(item['Prefix'])\n",
    "\n",
    "### EXAMPLE - REPLACE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb9fa4d",
   "metadata": {},
   "source": [
    "EXAMPLE - REPLACE\n",
    "\n",
    "Looking into the top-level S3 prefix of our dataset, we see that the data have been separated into training and evaluation datasets.\n",
    "\n",
    "EXAMPLE - REPLACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c582a4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "### EXAMPLE - REPLACE\n",
    "\n",
    "# List the key prefixes within the top level 'almrrc2021' prefix\n",
    "for item in s3.list_objects_v2(Bucket=bucket, Prefix='almrrc2021/', Delimiter='/', MaxKeys=10)['CommonPrefixes']:\n",
    "    print(item['Prefix'])\n",
    "\n",
    "### EXAMPLE - REPLACE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47268012",
   "metadata": {},
   "source": [
    "EXAMPLE - REPLACE\n",
    "\n",
    "The training and evaluation prefixes are similar in structure, and so we can look into the training portion to get a sense of the deeper structure of the dataset where the data objects reside.\n",
    "\n",
    "EXAMPLE - REPLACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963b40a5-67f0-4246-a49c-8c04d4efacca",
   "metadata": {},
   "outputs": [],
   "source": [
    "### EXAMPLE - REPLACE\n",
    "\n",
    "# List the keys within the 'almrrc2021/almrrc2021-data-training' prefix.\n",
    "for item in s3.list_objects_v2(Bucket=bucket, Prefix='almrrc2021/almrrc2021-data-training/', MaxKeys=100)['Contents']:\n",
    "    print(item['Key'])\n",
    "\n",
    "### EXAMPLE - REPLACE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7f4bcf-ec40-432f-a31f-4477efa205ee",
   "metadata": {
    "tags": []
   },
   "source": [
    "<!-- DATA PROVIDER INSTRUCTIONS\n",
    "This section is meant to orient users of your dataset to the formats present in your dataset, particularly if your dataset includes formats that may be unfamiliar to a general data scientist audience. This section should include:\n",
    "\n",
    "1. Explanation of data format(s) (very common formats can be very briefly described, while less common\n",
    "   or domain specific formats should include more explanation as well as links to official documentation)\n",
    "2. Explanation of why the data format was chosen for your dataset\n",
    "3. Recommendations around software and tooling to work with this data format\n",
    "4. Explanation of any dataset-specific aspects to your usage of the format\n",
    "5. Description of AWS services that may be useful to users working with your data\n",
    "DATA PROVIDER INSTRUCTIONS -->\n",
    "\n",
    "### Q: What data formats are present in your dataset? What kinds of data are stored using these formats? Can you give any advice for how you work with these data formats?\n",
    "\n",
    "Our dataset comes as a set of Cloud Optimized GeoTIFF (COG) files grouped by years from 2000. \n",
    "Each year includes 36 composites (3 per month). Each composite group contains the following datasets \n",
    "    Terra_MODIS_BX_Albedo.Noon_250m_YYYYMMDD.tif<br>\n",
    "    Terra_MODIS_BX_BRDF.Norm_250m_YYYYMMDD.tif<br>\n",
    "    Terra_MODIS_Index_BRDF.Norm_250m_YYYYMMDD.tif<br>\n",
    "    Terra_MODIS_NDVI_250m_YYYYMMDD.tif<br>\n",
    "    Terra_MODIS_VIS_BB_Albedo.Noon_250m_YYYYMMDD.tif<br>\n",
    "    Terra_MODIS_NIR_BB_Albedo.Noon_250m_YYYYMMDD.tif<br>\n",
    "    Terra_MODIS_SW_BB_Albedo.Noon_250m_YYYYMMDD.tif<br>\n",
    "    Terra_MODIS_Snow_Fraction_250m_YYYYMMDD.tif<br>\n",
    "    Terra_MODIS_Snow_GrainSize_1.0km_YYYYMMDD.tif<br>\n",
    "    Terra_MODIS_State_Mask_250m_YYYYMMDD.tif<br>\n",
    "where  YYYYMMDD labels year (YYYY), month (MM) and day (DD). <br>\n",
    "The reduced resolution (2.5km) false color browse images are also provided as Terra_MODIS_Albedo.Noon_Browse_FalseColor_RGB_YYYYMMDD_2.5km.tif\n",
    "\n",
    "COG datafiles can be processed using Python \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7362bd15",
   "metadata": {},
   "source": [
    "<!-- DATA PROVIDER INSTRUCTIONS\n",
    "The goal of this section is to demonstrate loading a portion of data from your dataset, and reveal something about its structure.\n",
    "1. Load an object from S3\n",
    "2. Show the structure of data in the object\n",
    "DATA PROVIDER INSTRUCTIONS -->\n",
    "\n",
    "### Q: Can you show us an example of downloading and loading data from your dataset?\n",
    "\n",
    "\n",
    "Load and view Broadband albedo for mid-summer 2012.\n",
    "\n",
    "Load Terra_MODIS_SW_BB_Albedo.Noon_250m_20120711.tif\n",
    "\n",
    "Load state mask (to select cloud-free pixels in necessary)\n",
    "\n",
    "Load Terra_MODIS_State_Mask_250m_20120711.tif\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd6c00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### EXAMPLE - REPLACE\n",
    "\n",
    "# First we'll load the data into a Python dictionary using the built-in json library\n",
    "\n",
    "file_key = \"almrrc2021/almrrc2021-data-training/model_build_inputs/package_data.json\"\n",
    "\n",
    "with s3.get_object(Bucket=bucket, Key=file_key)['Body'] as file_object:\n",
    "    package_data = json.load(file_object)\n",
    "\n",
    "### EXAMPLE - REPLACE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686af22a",
   "metadata": {},
   "source": [
    "EXAMPLE - REPLACE\n",
    "\n",
    "First, let's take a look at the keys in our newly-loaded dataset. Here we see that top-level keys correspond to route IDs.\n",
    "\n",
    "EXAMPLE - REPLACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed25fd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### EXAMPLE - REPLACE\n",
    "\n",
    "# pretty print a truncated list of keys in our dictionary\n",
    "pprint(list(package_data.keys())[:10])\n",
    "\n",
    "### EXAMPLE - REPLACE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9062e1",
   "metadata": {},
   "source": [
    "EXAMPLE - REPLACE\n",
    "\n",
    "Next we'll look at packages associated with the first route ID in our dataset to get a sense for the structure of this file. Here we note that each package ID in this file has dimensions as well as a planned service time duration.\n",
    "\n",
    "EXAMPLE - REPLACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7209334",
   "metadata": {},
   "outputs": [],
   "source": [
    "### EXAMPLE - REPLACE\n",
    "\n",
    "# pretty print the structure an individual route record\n",
    "pprint(package_data[\"RouteID_15baae2d-bf07-4967-956a-173d4036613f\"])\n",
    "\n",
    "### EXAMPLE - REPLACE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5717987e-c9c9-4c0f-9f2e-a8d761492625",
   "metadata": {},
   "source": [
    "<!-- DATA PROVIDER INSTRUCTIONS\n",
    "The goal here is to visualize some aspect of your dataset in order to help users understand it. In addition to helping users of your dataset understand the dataset, an additional goal is to impress!\n",
    "\n",
    "Please demonstrate any data preprocessing or reshaping required for your visualization(s).\n",
    "\n",
    "https://www.reddit.com/r/dataisbeautiful/ for inspiration.\n",
    "DATA PROVIDER INSTRUCTIONS -->\n",
    "\n",
    "### Q: A picture is worth a thousand words. Show us a visual (or several!) from your dataset that either illustrates something informative about your dataset, or that you think might excite someone to dig in further.\n",
    "\n",
    "Create false-color composite image of the area for mid-summer 2012 <br>\n",
    "    Load Terra_MODIS_B1_BRDF.Norm_250m_20000711.tif as BLUE(B) band <br>\n",
    "    Load Terra_MODIS_B2_BRDF.Norm_250m_20000711.tif as GREE(G) band <br>\n",
    "    Load Terra_MODIS_B6_BRDF.Norm_250m_20000711.tif as RED(R) band <br>\n",
    "This RGB picture will show the area covered by our dataset and the distribution of typical surface features (land, water, snow, vegetation, deserted areas)\n",
    "    Load Terra_MODIS_SW_BB_Albedo.Noon_250m_20120711.tif file to see the corresponding values of shortwave (SW) broadband albedo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9002c5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "### EXAMPLE - REPLACE\n",
    "\n",
    "# Create a function to flatten our data structure\n",
    "def flatten_package_data(data):\n",
    "    flattened = []\n",
    "    \n",
    "    for route_key, route_data in data.items():\n",
    "        route_id = route_key.split('_')[1]  # Extract RouteID part\n",
    "        \n",
    "        # Iterate through all zone dictionaries (like 'AH')\n",
    "        for zone_data in route_data.values():\n",
    "            # Iterate through package dictionaries\n",
    "            for package_key, package_info in zone_data.items():\n",
    "                package_id = package_key.split('_')[1]  # Extract PackageID part\n",
    "                \n",
    "                flattened.append({\n",
    "                    'RouteID': route_id,\n",
    "                    'PackageID': package_id,\n",
    "                    'depth_cm': package_info['dimensions']['depth_cm'],\n",
    "                    'height_cm': package_info['dimensions']['height_cm'],\n",
    "                    'width_cm': package_info['dimensions']['width_cm'],\n",
    "                    'planned_service_time_seconds': package_info['planned_service_time_seconds']\n",
    "                })\n",
    "    \n",
    "    return polars.DataFrame(flattened)\n",
    "\n",
    "# Convert to Polars DataFrame\n",
    "df = flatten_package_data(package_data)\n",
    "\n",
    "### EXAMPLE - REPLACE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf5cdf3",
   "metadata": {},
   "source": [
    "EXAMPLE - REPLACE\n",
    "\n",
    "Now let's take a look at the first few records of our newly flattened dataset.\n",
    "\n",
    "EXAMPLE - REPLACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef33976",
   "metadata": {},
   "outputs": [],
   "source": [
    "### EXAMPLE - REPLACE\n",
    "\n",
    "# Print first few rows of our newly flattened dataset\n",
    "df.head()\n",
    "\n",
    "### EXAMPLE - REPLACE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50bd9347",
   "metadata": {},
   "source": [
    "EXAMPLE - REPLACE\n",
    "\n",
    "Now let's add another column, volume_cm3, that gives us the volume of each package.\n",
    "\n",
    "EXAMPLE - REPLACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008c53ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "### EXAMPLE - REPLACE\n",
    "\n",
    "# Calculate volume by multiplying length, width, and depth dimensions\n",
    "df = df.with_columns(\n",
    "    (polars.col('depth_cm') * polars.col('height_cm') * polars.col('width_cm')).alias('volume_cm3')\n",
    ")\n",
    "\n",
    "df.head()\n",
    "\n",
    "### EXAMPLE - REPLACE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303cbc82",
   "metadata": {},
   "source": [
    "EXAMPLE - REPLACE\n",
    "\n",
    "Now we can plot the distribution of package volumes for the entirety of our dataset.\n",
    "\n",
    "EXAMPLE - REPLACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df75c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "### EXAMPLE - REPLACE\n",
    "\n",
    "\n",
    "# Plot using matplotlib\n",
    "# Set figure size and DPI for better resolution\n",
    "plt.figure(figsize=(12, 7), dpi=100, facecolor='white')\n",
    "\n",
    "# Create histogram with custom styling\n",
    "plt.hist(df['volume_cm3'], \n",
    "         bins=30,\n",
    "         color='#3498db',    # Nice blue color\n",
    "         edgecolor='white',\n",
    "         linewidth=1.2,\n",
    "         alpha=0.8)\n",
    "\n",
    "# Customize title and labels\n",
    "plt.title('Distribution of Package Volumes', \n",
    "         fontsize=16, \n",
    "         pad=20, \n",
    "         fontweight='bold')\n",
    "plt.xlabel('Volume (cmÂ³)', fontsize=12, labelpad=10)\n",
    "plt.ylabel('Count', fontsize=12, labelpad=10)\n",
    "\n",
    "# Add grid with custom styling\n",
    "plt.grid(True, linestyle='--', alpha=0.3, color='gray')\n",
    "\n",
    "# Customize axes and background\n",
    "ax = plt.gca()\n",
    "ax.set_facecolor('#f8f9fa')  # Light gray background\n",
    "ax.spines['top'].set_visible(False)    # Remove top border\n",
    "ax.spines['right'].set_visible(False)  # Remove right border\n",
    "ax.spines['left'].set_linewidth(0.5)   # Thin left border\n",
    "ax.spines['bottom'].set_linewidth(0.5) # Thin bottom border\n",
    "\n",
    "# Adjust layout to prevent label clipping\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "### EXAMPLE - REPLACE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a2f5a5",
   "metadata": {},
   "source": [
    "EXAMPLE - REPLACE\n",
    "\n",
    "Now, let's take a look and see if we can get a sense of how many packages there are to deliver per route.\n",
    "\n",
    "EXAMPLE - REPLACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5e3e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "### EXAMPLE - REPLACE\n",
    "\n",
    "# Group by RouteID and count PackageID\n",
    "packages_per_route = df.group_by('RouteID').agg(\n",
    "    polars.col('PackageID').count()\n",
    ").get_column('PackageID')\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(12, 7), dpi=100, facecolor='white')\n",
    "\n",
    "plt.hist(packages_per_route, \n",
    "         bins=30,\n",
    "         color='#3498db',\n",
    "         edgecolor='white',\n",
    "         linewidth=1.2,\n",
    "         alpha=0.8)\n",
    "\n",
    "plt.title('Distribution of Packages per Route', \n",
    "         fontsize=16, \n",
    "         pad=20, \n",
    "         fontweight='bold')\n",
    "plt.xlabel('Number of Packages', fontsize=12, labelpad=10)\n",
    "plt.ylabel('Number of Routes', fontsize=12, labelpad=10)\n",
    "\n",
    "plt.grid(True, linestyle='--', alpha=0.3, color='gray')\n",
    "\n",
    "# Customize axes and background\n",
    "ax = plt.gca()\n",
    "ax.set_facecolor('#f8f9fa')\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['left'].set_linewidth(0.5)\n",
    "ax.spines['bottom'].set_linewidth(0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print some summary statistics using Polars\n",
    "print(f\"Average packages per route: {packages_per_route.mean():.1f}\")\n",
    "print(f\"Median packages per route: {packages_per_route.median():.1f}\")\n",
    "print(f\"Min packages per route: {packages_per_route.min()}\")\n",
    "print(f\"Max packages per route: {packages_per_route.max()}\")\n",
    "\n",
    "### EXAMPLE - REPLACE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183c8b85-ed1c-4f2c-bd0e-fbfbc67c4723",
   "metadata": {},
   "source": [
    "<!-- DATA PROVIDER INSTRUCTIONS\n",
    "This section is less prescriptive / freeform than previous sections. The goal here is to show an opinionated example of answering a question using your data. The scale of your dataset may preclude a full example, and so feel free to limit the scope of this example (e.g. work on a subset of data). Users should be able to replicate your example in this notebook, and get a sense of how they would scale up.\n",
    "\n",
    "A \"toy\" example is better than no example.\n",
    "\n",
    "Ideally, your example would:\n",
    "1. Transmit some of your domain & dataset experience to the reader, drawing on your own work as much as possible\n",
    "2. Provide a jumping off point for users to extend your work, and do novel work of their own.\n",
    "\n",
    "DATA PROVIDER INSTRUCTIONS -->\n",
    "\n",
    "### Q: What is one question that you have answered using these data? Can you show us how you came to that answer?\n",
    "\n",
    "These data have been used to study the Minimum Snow/Ice (MSI) extent variations, quantifying the rate of declining the Artic and alpine glaciated areas of North America, as well as climate impacts of radiative forcing driven by agricultural land-use and land-cover changes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf645724-3108-4ada-a832-10b3431eb8e2",
   "metadata": {},
   "source": [
    "<!-- DATA PROVIDER INSTRUCTIONS\n",
    "This section is, like the previous one, intended to be freeform / non-prescriptive. The goal here is to provide a challenge to the community to do something novel with your dataset. That can either be novel in terms of the task, or novel in terms of methodological or computational approach.\n",
    "\n",
    "Another way to consider this section, is as a wishlist. If you were less constrained by time, cost, skill, etc., what would you like to see achieved using these data? \n",
    "\n",
    "The challenge should, however, be somewhat realistic. A challenge that assumes e.g. original data collection, is likely to go unanswered.\n",
    "DATA PROVIDER INSTRUCTIONS -->\n",
    "\n",
    "### Q: What is one unanswered question that you think could be answered using these data? Do you have any recommendations or advice for someone wanting to answer this question?\n",
    "\n",
    "What new can we learn from these data about reflectance and albedo for water/ocean surface. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
